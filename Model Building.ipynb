{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Required Libraries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from keras.applications.vgg16 import VGG16,preprocess_input\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Dropout, add\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Loading Our Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134260544 (512.16 MB)\n",
      "Trainable params: 134260544 (512.16 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=VGG16() # Loading pre-trained VGG-16 Model\n",
    "model=Model(inputs=model.inputs, outputs=model.layers[-2] .output) # Restructuring the Model\n",
    "model.summary() # Model Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Loading Our Dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir=\"E:/Data Science/Projects/Dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory=os.path.join(base_dir+'Images')\n",
    "\n",
    "# Feature Extraction from Images\n",
    "features={}\n",
    "\n",
    "for img_name in tqdm(os.listdir(directory)):\n",
    "\n",
    "    # Loading images from file\n",
    "    img_path=os.path.join(directory,img_name)\n",
    "    image=load_img(img_path, target_size=(224,224))\n",
    "\n",
    "    # Converting image into numpy array\n",
    "    image=img_to_array(image)\n",
    "\n",
    "    # Reshaping the Image\n",
    "    image=image.reshape(1,image.shape[0],image.shape[1],image.shape[2])\n",
    "\n",
    "    # Preprocessing the image\n",
    "    image=preprocess_input(image)\n",
    "\n",
    "    # Feature Extraction\n",
    "    feature=model.predict(image,verbose=0)\n",
    "    \n",
    "    # Getting image ID\n",
    "    img_id=img_name.split('.')[0]\n",
    "\n",
    "    # Storing Features\n",
    "    features[img_id]=feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing Feature in Pickle \n",
    "# pickle.dump(features, open('Extracted_Features.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Extracted_Features.pkl','rb') as f:\n",
    "    features=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Caption Data\n",
    "with open(os.path.join(base_dir,'captions.txt'),'r') as f:\n",
    "    next(f)\n",
    "    caption_doc=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40456/40456 [00:00<00:00, 56149.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# Mapping Of Image to Caption\n",
    "mapping={}\n",
    "for line in tqdm(caption_doc.split('\\n')):\n",
    "    tokens=line.split(',')\n",
    "    if len(line)<2:\n",
    "        continue\n",
    "    img_name, captions=tokens[0],tokens[1:]\n",
    "    img_id=img_name.split('.')[0]\n",
    "    caption=\" \".join(captions)\n",
    "\n",
    "    if img_id not in mapping:\n",
    "        mapping[img_id]=[]\n",
    "    \n",
    "    mapping[img_id].append(caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Preprocessing Text Data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(mapping):\n",
    "    for key,captions in mapping.items():\n",
    "        for i in range(len(captions)):\n",
    "            caption=captions[i]\n",
    "            caption=caption.lower()\n",
    "            caption=caption.replace('[^A-Za-z]','')\n",
    "            caption=caption.replace('\\s+',' ')\n",
    "            caption='startseq '+\" \".join([word for word in caption.split() if len(word)>1])+' endseq'\n",
    "            captions[i]=caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A child in a pink dress is climbing up a set of stairs in an entry way .',\n",
       " 'A girl going into a wooden building .',\n",
       " 'A little girl climbing into a wooden playhouse .',\n",
       " 'A little girl climbing the stairs to her playhouse .',\n",
       " 'A little girl in a pink dress going into a wooden cabin .']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping['1000268201_693b08cb0e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['startseq child in pink dress is climbing up set of stairs in an entry way endseq',\n",
       " 'startseq girl going into wooden building endseq',\n",
       " 'startseq little girl climbing into wooden playhouse endseq',\n",
       " 'startseq little girl climbing the stairs to her playhouse endseq',\n",
       " 'startseq little girl in pink dress going into wooden cabin endseq']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean(mapping)\n",
    "mapping['1000268201_693b08cb0e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_captions=[]\n",
    "for key in mapping:\n",
    "    for caption in mapping[key]:\n",
    "        all_captions.append(caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Tokenization*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(all_captions)\n",
    "vocab_size=len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8485"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting maximum length of a caption\n",
    "max_length=max(len(caption.split()) for caption in all_captions)\n",
    "max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Splitting the Data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ids=list(mapping.keys())\n",
    "split=int(len(img_ids)*0.90)\n",
    "train=img_ids[:split]\n",
    "test=img_ids[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Creating Data Generator*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(data_keys, mapping, features, tokenizer, max_lenght, vocab_size, batch_size):\n",
    "    x1, x2, y = list(), list(), list()\n",
    "    n = 0\n",
    "    while 1:\n",
    "        for key in data_keys:\n",
    "            n += 1\n",
    "            captions = mapping[key]\n",
    "            for caption in captions:\n",
    "                seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "\n",
    "                for i in range(1, len(seq)):\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=max_lenght)[0]\n",
    "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "\n",
    "                    x1.append(features[key][0])\n",
    "                    x2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "\n",
    "            if n == batch_size:\n",
    "                x1, x2, y = np.array(x1), np.array(x2), np.array(y)\n",
    "                yield {\"image\":x1, \"text\":x2}, y\n",
    "                x1, x2, y = list(), list(), list()\n",
    "                n = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Model Building*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Encoder Model*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Image Feature Layers*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs1=Input(shape=(4096,),name=\"image\")\n",
    "fe1=Dropout(0.4)(inputs1)\n",
    "fe2=Dense(256,activation='relu')(fe1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Sequence Feature Layer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs2=Input(shape=(max_length,),name=\"text\")\n",
    "se1=Embedding(vocab_size,256,mask_zero=True)(inputs2)\n",
    "se2=Dropout(0.4)(se1)\n",
    "se3=LSTM(256)(se2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Decoder Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder1=add([fe2,se3])\n",
    "decoder2=Dense(256,activation='relu')(decoder1)\n",
    "outputs=Dense(vocab_size, activation='softmax')(decoder2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Model(inputs=[inputs1, inputs2], outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Model Compilation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Plot Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_model(model,show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Model Training*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227/227 [==============================] - 670s 3s/step - loss: 5.1854\n"
     ]
    }
   ],
   "source": [
    "epochs=20\n",
    "batch_size=32\n",
    "steps=len(train)//batch_size\n",
    "\n",
    "for i in range(epochs):\n",
    "    generator=data_generator(train,mapping, features, tokenizer, max_length,vocab_size,batch_size)\n",
    "    model.fit(generator,epochs=1,steps_per_epoch=steps,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Model Saving*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model=load_model('D:/model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Generate Caption for an Image*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indx_to_word(integer,tokenizer):\n",
    "    for word,index in tokenizer.word_index.items():\n",
    "        if index==integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_caption(model,image,tokenizer,max_length):\n",
    "    in_text='startseq'\n",
    "    for i in range(max_length):\n",
    "        seq=tokenizer.texts_to_sequences([in_text])[0]\n",
    "        pad_seq=pad_sequences([seq],max_length)\n",
    "        y_pred=model.predict([image,pad_seq],verbose=0)\n",
    "        y_pred=np.argmax(y_pred)\n",
    "        word=indx_to_word(y_pred,tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        in_text+=\" \"+word\n",
    "        if word=='endseq':\n",
    "            break\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Model Evaluation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual, predicted=list(),list()\n",
    "for key in tqdm(test):\n",
    "    captions=mapping[key]\n",
    "    y_pred=predict_caption(model,features[key],tokenizer,max_length)\n",
    "    y_pred=y_pred.split()\n",
    "    actual_caption=[caption.split for caption in captions]\n",
    "    actual.append(actual_caption)\n",
    "    predicted.append(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('BLEU Score-1:',corpus_bleu(actual,predicted,weights=(1,0,0,0)))\n",
    "print('BLEU Score-2:',corpus_bleu(actual,predicted,weights=(0.5,0.5,0,0)))\n",
    "print('BLEU Score-3:',corpus_bleu(actual,predicted,weights=(0.25,0.25,0.25,0.25)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Visualizing The Results*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name=\"10815824_2997e03d76.jpg\"\n",
    "def visualize(img_name):\n",
    "    img_id=img_name.split('.')[0]\n",
    "    img_path=os.path.join(base_dir,\"Images\",img_name)\n",
    "    image=Image.open(img_path)\n",
    "    captions=mapping[img_id]\n",
    "\n",
    "    print('Actual')\n",
    "    for caption in captions:\n",
    "        print(caption)\n",
    "\n",
    "    y_pred=predict_caption(model,features[img_id],tokenizer,max_length)\n",
    "\n",
    "    print('Predicted')\n",
    "\n",
    "    print(y_pred)\n",
    "\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(img_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
